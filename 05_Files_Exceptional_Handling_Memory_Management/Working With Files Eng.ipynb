{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ecd4b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"test.txt\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7010bdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\prasa\\\\Desktop\\\\Desktop\\\\DataScience_Masters\\\\05_Files_Exceptional_Handling_Memory_Management'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bfd590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 5447-B14C\n",
      "\n",
      " Directory of C:\\Users\\prasa\\Desktop\\Desktop\\DataScience_Masters\\05_Files_Exceptional_Handling_Memory_Management\n",
      "\n",
      "29-07-2024  13:43    <DIR>          .\n",
      "29-07-2024  13:40    <DIR>          ..\n",
      "29-07-2024  13:41    <DIR>          .ipynb_checkpoints\n",
      "29-07-2024  13:43                 0 test.txt\n",
      "29-07-2024  13:43               756 Working With Files Eng.ipynb\n",
      "               2 File(s)            756 bytes\n",
      "               3 Dir(s)  202,917,044,224 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f77b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.write(\"This is my first write Operation in the file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1674643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "972c69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"test.txt\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf9982cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2399"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.write(\"\"\"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It combines aspects of statistics, computer science, and domain-specific knowledge to analyze and interpret complex data.\n",
    "\n",
    "Here are the key steps and components typically involved in a data science workflow:\n",
    "\n",
    "Data Collection: Gathering data from various sources, which can include databases, web scraping, APIs, and manual entry.\n",
    "\n",
    "Data Cleaning: Preparing the data for analysis by handling missing values, removing duplicates, correcting errors, and formatting the data properly.\n",
    "\n",
    "Data Exploration and Visualization: Using statistical methods and visualization tools to understand the data's structure, patterns, and relationships. Common tools include Python libraries such as Matplotlib, Seaborn, and Plotly, as well as R libraries like ggplot2.\n",
    "\n",
    "Feature Engineering: Creating new features from existing data that can help improve the performance of machine learning models. This may involve transforming variables, combining multiple features, or creating dummy variables.\n",
    "\n",
    "Model Selection and Training: Choosing appropriate machine learning models based on the problem type (regression, classification, clustering, etc.) and training them on the prepared dataset. Common algorithms include linear regression, decision trees, random forests, support vector machines, and neural networks.\n",
    "\n",
    "Model Evaluation: Assessing the performance of the trained models using metrics such as accuracy, precision, recall, F1 score, and AUC-ROC for classification tasks, and RMSE, MAE, and R² for regression tasks.\n",
    "\n",
    "Model Tuning: Improving model performance through hyperparameter tuning, cross-validation, and feature selection techniques.\n",
    "\n",
    "Deployment: Integrating the trained model into a production environment where it can make predictions on new data. This often involves creating APIs, using containerization tools like Docker, and deploying to cloud platforms such as AWS, Google Cloud, or Azure.\n",
    "\n",
    "Monitoring and Maintenance: Continuously monitoring the model's performance in production and updating it as needed to ensure it remains accurate and relevant over time.\n",
    "\n",
    "Communication and Reporting: Presenting the findings and insights to stakeholders through reports, dashboards, and visualizations to inform decision-making.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a119103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cce7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"test.txt\",'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d18044c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.write(\"\"\"Example Projects\n",
    "Predictive Modeling: Building models to predict future outcomes based on historical data (e.g., sales forecasting, customer churn prediction).\n",
    "Recommendation Systems: Creating systems that recommend products or services to users based on their preferences and behavior (e.g., movie recommendations, product recommendations).\n",
    "Sentiment Analysis: Analyzing text data to determine the sentiment (positive, negative, neutral) expressed in it (e.g., social media sentiment analysis).\n",
    "Image Classification: Using deep learning to classify images into predefined categories (e.g., detecting objects in photos, medical image diagnosis).\n",
    "Anomaly Detection: Identifying unusual patterns or outliers in data (e.g., fraud detection, network intrusion detection\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1709c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68238d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=open(\"test.txt\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f247c042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It combines aspects of statistics, computer science, and domain-specific knowledge to analyze and interpret complex data.\\n\\nHere are the key steps and components typically involved in a data science workflow:\\n\\nData Collection: Gathering data from various sources, which can include databases, web scraping, APIs, and manual entry.\\n\\nData Cleaning: Preparing the data for analysis by handling missing values, removing duplicates, correcting errors, and formatting the data properly.\\n\\nData Exploration and Visualization: Using statistical methods and visualization tools to understand the data's structure, patterns, and relationships. Common tools include Python libraries such as Matplotlib, Seaborn, and Plotly, as well as R libraries like ggplot2.\\n\\nFeature Engineering: Creating new features from existing data that can help improve the performance of machine learning models. This may involve transforming variables, combining multiple features, or creating dummy variables.\\n\\nModel Selection and Training: Choosing appropriate machine learning models based on the problem type (regression, classification, clustering, etc.) and training them on the prepared dataset. Common algorithms include linear regression, decision trees, random forests, support vector machines, and neural networks.\\n\\nModel Evaluation: Assessing the performance of the trained models using metrics such as accuracy, precision, recall, F1 score, and AUC-ROC for classification tasks, and RMSE, MAE, and R² for regression tasks.\\n\\nModel Tuning: Improving model performance through hyperparameter tuning, cross-validation, and feature selection techniques.\\n\\nDeployment: Integrating the trained model into a production environment where it can make predictions on new data. This often involves creating APIs, using containerization tools like Docker, and deploying to cloud platforms such as AWS, Google Cloud, or Azure.\\n\\nMonitoring and Maintenance: Continuously monitoring the model's performance in production and updating it as needed to ensure it remains accurate and relevant over time.\\n\\nCommunication and Reporting: Presenting the findings and insights to stakeholders through reports, dashboards, and visualizations to inform decision-making.Example Projects\\nPredictive Modeling: Building models to predict future outcomes based on historical data (e.g., sales forecasting, customer churn prediction).\\nRecommendation Systems: Creating systems that recommend products or services to users based on their preferences and behavior (e.g., movie recommendations, product recommendations).\\nSentiment Analysis: Analyzing text data to determine the sentiment (positive, negative, neutral) expressed in it (e.g., social media sentiment analysis).\\nImage Classification: Using deep learning to classify images into predefined categories (e.g., detecting objects in photos, medical image diagnosis).\\nAnomaly Detection: Identifying unusual patterns or outliers in data (e.g., fraud detection, network intrusion detection\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30f01114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5d2321d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "407bd4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It combines aspects of statistics, computer science, and domain-specific knowledge to analyze and interpret complex data.\\n\\nHere are the key steps and components typically involved in a data science workflow:\\n\\nData Collection: Gathering data from various sources, which can include databases, web scraping, APIs, and manual entry.\\n\\nData Cleaning: Preparing the data for analysis by handling missing values, removing duplicates, correcting errors, and formatting the data properly.\\n\\nData Exploration and Visualization: Using statistical methods and visualization tools to understand the data's structure, patterns, and relationships. Common tools include Python libraries such as Matplotlib, Seaborn, and Plotly, as well as R libraries like ggplot2.\\n\\nFeature Engineering: Creating new features from existing data that can help improve the performance of machine learning models. This may involve transforming variables, combining multiple features, or creating dummy variables.\\n\\nModel Selection and Training: Choosing appropriate machine learning models based on the problem type (regression, classification, clustering, etc.) and training them on the prepared dataset. Common algorithms include linear regression, decision trees, random forests, support vector machines, and neural networks.\\n\\nModel Evaluation: Assessing the performance of the trained models using metrics such as accuracy, precision, recall, F1 score, and AUC-ROC for classification tasks, and RMSE, MAE, and R² for regression tasks.\\n\\nModel Tuning: Improving model performance through hyperparameter tuning, cross-validation, and feature selection techniques.\\n\\nDeployment: Integrating the trained model into a production environment where it can make predictions on new data. This often involves creating APIs, using containerization tools like Docker, and deploying to cloud platforms such as AWS, Google Cloud, or Azure.\\n\\nMonitoring and Maintenance: Continuously monitoring the model's performance in production and updating it as needed to ensure it remains accurate and relevant over time.\\n\\nCommunication and Reporting: Presenting the findings and insights to stakeholders through reports, dashboards, and visualizations to inform decision-making.Example Projects\\nPredictive Modeling: Building models to predict future outcomes based on historical data (e.g., sales forecasting, customer churn prediction).\\nRecommendation Systems: Creating systems that recommend products or services to users based on their preferences and behavior (e.g., movie recommendations, product recommendations).\\nSentiment Analysis: Analyzing text data to determine the sentiment (positive, negative, neutral) expressed in it (e.g., social media sentiment analysis).\\nImage Classification: Using deep learning to classify images into predefined categories (e.g., detecting objects in photos, medical image diagnosis).\\nAnomaly Detection: Identifying unusual patterns or outliers in data (e.g., fraud detection, network intrusion detection\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c3d4ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c44145f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.seek(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d51a1ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"systems to extract knowledge and insights from structured and unstructured data. It combines aspects of statistics, computer science, and domain-specific knowledge to analyze and interpret complex data.\\n\\nHere are the key steps and components typically involved in a data science workflow:\\n\\nData Collection: Gathering data from various sources, which can include databases, web scraping, APIs, and manual entry.\\n\\nData Cleaning: Preparing the data for analysis by handling missing values, removing duplicates, correcting errors, and formatting the data properly.\\n\\nData Exploration and Visualization: Using statistical methods and visualization tools to understand the data's structure, patterns, and relationships. Common tools include Python libraries such as Matplotlib, Seaborn, and Plotly, as well as R libraries like ggplot2.\\n\\nFeature Engineering: Creating new features from existing data that can help improve the performance of machine learning models. This may involve transforming variables, combining multiple features, or creating dummy variables.\\n\\nModel Selection and Training: Choosing appropriate machine learning models based on the problem type (regression, classification, clustering, etc.) and training them on the prepared dataset. Common algorithms include linear regression, decision trees, random forests, support vector machines, and neural networks.\\n\\nModel Evaluation: Assessing the performance of the trained models using metrics such as accuracy, precision, recall, F1 score, and AUC-ROC for classification tasks, and RMSE, MAE, and R² for regression tasks.\\n\\nModel Tuning: Improving model performance through hyperparameter tuning, cross-validation, and feature selection techniques.\\n\\nDeployment: Integrating the trained model into a production environment where it can make predictions on new data. This often involves creating APIs, using containerization tools like Docker, and deploying to cloud platforms such as AWS, Google Cloud, or Azure.\\n\\nMonitoring and Maintenance: Continuously monitoring the model's performance in production and updating it as needed to ensure it remains accurate and relevant over time.\\n\\nCommunication and Reporting: Presenting the findings and insights to stakeholders through reports, dashboards, and visualizations to inform decision-making.Example Projects\\nPredictive Modeling: Building models to predict future outcomes based on historical data (e.g., sales forecasting, customer churn prediction).\\nRecommendation Systems: Creating systems that recommend products or services to users based on their preferences and behavior (e.g., movie recommendations, product recommendations).\\nSentiment Analysis: Analyzing text data to determine the sentiment (positive, negative, neutral) expressed in it (e.g., social media sentiment analysis).\\nImage Classification: Using deep learning to classify images into predefined categories (e.g., detecting objects in photos, medical image diagnosis).\\nAnomaly Detection: Identifying unusual patterns or outliers in data (e.g., fraud detection, network intrusion detection\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54681da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=open(\"test.txt\",'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffed4086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It combines aspects of statistics, computer science, and domain-specific knowledge to analyze and interpret complex data.\n",
      "\n",
      "\n",
      "\n",
      "Here are the key steps and components typically involved in a data science workflow:\n",
      "\n",
      "\n",
      "\n",
      "Data Collection: Gathering data from various sources, which can include databases, web scraping, APIs, and manual entry.\n",
      "\n",
      "\n",
      "\n",
      "Data Cleaning: Preparing the data for analysis by handling missing values, removing duplicates, correcting errors, and formatting the data properly.\n",
      "\n",
      "\n",
      "\n",
      "Data Exploration and Visualization: Using statistical methods and visualization tools to understand the data's structure, patterns, and relationships. Common tools include Python libraries such as Matplotlib, Seaborn, and Plotly, as well as R libraries like ggplot2.\n",
      "\n",
      "\n",
      "\n",
      "Feature Engineering: Creating new features from existing data that can help improve the performance of machine learning models. This may involve transforming variables, combining multiple features, or creating dummy variables.\n",
      "\n",
      "\n",
      "\n",
      "Model Selection and Training: Choosing appropriate machine learning models based on the problem type (regression, classification, clustering, etc.) and training them on the prepared dataset. Common algorithms include linear regression, decision trees, random forests, support vector machines, and neural networks.\n",
      "\n",
      "\n",
      "\n",
      "Model Evaluation: Assessing the performance of the trained models using metrics such as accuracy, precision, recall, F1 score, and AUC-ROC for classification tasks, and RMSE, MAE, and R² for regression tasks.\n",
      "\n",
      "\n",
      "\n",
      "Model Tuning: Improving model performance through hyperparameter tuning, cross-validation, and feature selection techniques.\n",
      "\n",
      "\n",
      "\n",
      "Deployment: Integrating the trained model into a production environment where it can make predictions on new data. This often involves creating APIs, using containerization tools like Docker, and deploying to cloud platforms such as AWS, Google Cloud, or Azure.\n",
      "\n",
      "\n",
      "\n",
      "Monitoring and Maintenance: Continuously monitoring the model's performance in production and updating it as needed to ensure it remains accurate and relevant over time.\n",
      "\n",
      "\n",
      "\n",
      "Communication and Reporting: Presenting the findings and insights to stakeholders through reports, dashboards, and visualizations to inform decision-making.Example Projects\n",
      "\n",
      "Predictive Modeling: Building models to predict future outcomes based on historical data (e.g., sales forecasting, customer churn prediction).\n",
      "\n",
      "Recommendation Systems: Creating systems that recommend products or services to users based on their preferences and behavior (e.g., movie recommendations, product recommendations).\n",
      "\n",
      "Sentiment Analysis: Analyzing text data to determine the sentiment (positive, negative, neutral) expressed in it (e.g., social media sentiment analysis).\n",
      "\n",
      "Image Classification: Using deep learning to classify images into predefined categories (e.g., detecting objects in photos, medical image diagnosis).\n",
      "\n",
      "Anomaly Detection: Identifying unusual patterns or outliers in data (e.g., fraud detection, network intrusion detection\n"
     ]
    }
   ],
   "source": [
    "for i in data1:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e7d7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b71ba3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3191"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.getsize('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57d44174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66c12a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test1.txt'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy('test.txt','test1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a88e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('test1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e57cd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It combines aspects of statistics, computer science, and domain-specific knowledge to analyze and interpret complex data.\n",
      "\n",
      "Here are the key steps and components typically involved in a data science workflow:\n",
      "\n",
      "Data Collection: Gathering data from various sources, which can include databases, web scraping, APIs, and manual entry.\n",
      "\n",
      "Data Cleaning: Preparing the data for analysis by handling missing values, removing duplicates, correcting errors, and formatting the data properly.\n",
      "\n",
      "Data Exploration and Visualization: Using statistical methods and visualization tools to understand the data's structure, patterns, and relationships. Common tools include Python libraries such as Matplotlib, Seaborn, and Plotly, as well as R libraries like ggplot2.\n",
      "\n",
      "Feature Engineering: Creating new features from existing data that can help improve the performance of machine learning models. This may involve transforming variables, combining multiple features, or creating dummy variables.\n",
      "\n",
      "Model Selection and Training: Choosing appropriate machine learning models based on the problem type (regression, classification, clustering, etc.) and training them on the prepared dataset. Common algorithms include linear regression, decision trees, random forests, support vector machines, and neural networks.\n",
      "\n",
      "Model Evaluation: Assessing the performance of the trained models using metrics such as accuracy, precision, recall, F1 score, and AUC-ROC for classification tasks, and RMSE, MAE, and R² for regression tasks.\n",
      "\n",
      "Model Tuning: Improving model performance through hyperparameter tuning, cross-validation, and feature selection techniques.\n",
      "\n",
      "Deployment: Integrating the trained model into a production environment where it can make predictions on new data. This often involves creating APIs, using containerization tools like Docker, and deploying to cloud platforms such as AWS, Google Cloud, or Azure.\n",
      "\n",
      "Monitoring and Maintenance: Continuously monitoring the model's performance in production and updating it as needed to ensure it remains accurate and relevant over time.\n",
      "\n",
      "Communication and Reporting: Presenting the findings and insights to stakeholders through reports, dashboards, and visualizations to inform decision-making.Example Projects\n",
      "Predictive Modeling: Building models to predict future outcomes based on historical data (e.g., sales forecasting, customer churn prediction).\n",
      "Recommendation Systems: Creating systems that recommend products or services to users based on their preferences and behavior (e.g., movie recommendations, product recommendations).\n",
      "Sentiment Analysis: Analyzing text data to determine the sentiment (positive, negative, neutral) expressed in it (e.g., social media sentiment analysis).\n",
      "Image Classification: Using deep learning to classify images into predefined categories (e.g., detecting objects in photos, medical image diagnosis).\n",
      "Anomaly Detection: Identifying unusual patterns or outliers in data (e.g., fraud detection, network intrusion detection\n"
     ]
    }
   ],
   "source": [
    "with open('test.txt','r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a15431c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()\n",
    "data1.close()\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "180d99c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()\n",
    "os.rename(\"test.txt\",\"new_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec0bef",
   "metadata": {},
   "source": [
    "# File Reading and Writing Eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a99fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\n",
    "    \"name\":\"Prasath\",\n",
    "    \"mail_id\":\"prasath@gmail.com\",\n",
    "    \"phone_number\":9898465151,\n",
    "    \"subject\":[\"Data Science\",\"Big Data\",\"Data analytics\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c4b5398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Prasath',\n",
       " 'mail_id': 'prasath@gmail.com',\n",
       " 'phone_number': 9898465151,\n",
       " 'subject': ['Data Science', 'Big Data', 'Data analytics']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e45c5157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd277c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test1.json','w') as f:\n",
    "    json.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "485e93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test1.json','r') as f:\n",
    "    data=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8e702b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Prasath',\n",
       " 'mail_id': 'prasath@gmail.com',\n",
       " 'phone_number': 9898465151,\n",
       " 'subject': ['Data Science', 'Big Data', 'Data analytics']}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75c8b2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Science', 'Big Data', 'Data analytics']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0accc835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Big Data'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subject'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2ad421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[['name','email_id','number'],['prasath','prasath@gmail.com',8974916598],['krish','krish@gmail.com',68651895698]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ade7c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48296a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text3.csv','w') as f:\n",
    "    w=csv.writer(f)\n",
    "    for i in data:\n",
    "        w.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1bb245a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'email_id', 'number']\n",
      "[]\n",
      "['prasath', 'prasath@gmail.com', '8974916598']\n",
      "[]\n",
      "['krish', 'krish@gmail.com', '68651895698']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "with open('text3.csv','r') as f:\n",
    "    read=csv.reader(f)\n",
    "    for i in read:\n",
    "        print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8692bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test4.bin','wb') as f:\n",
    "    f.write(b\"\\x01\\x02\\x03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6f179959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x01\\x02\\x03'\n"
     ]
    }
   ],
   "source": [
    "with open('test4.bin','rb') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e74fb17",
   "metadata": {},
   "source": [
    "# Buffered Read and Write Eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f285b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0018676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt','wb') as f:\n",
    "    file=io.BufferedWriter(f)\n",
    "    file.write(b\"this is my buffer write\\n\")\n",
    "    file.write(b\"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It combines aspects of statistics, computer science, and domain-specific knowledge to analyze and interpret complex data.\")\n",
    "    file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e6d1ce14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'this is my buffer write\\nData science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured d'\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\",'rb') as f:\n",
    "    file=io.BufferedReader(f)\n",
    "    data=file.read(200)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b532b83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1ad64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd585d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3991e832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
